#!/usr/bin/env python
"""
Intelligent JavaSecurityDetector Test

This test demonstrates the full JavaSecurityDetector using LLM-powered
intelligence to analyze the vulnerable Java project. It combines:
- Pattern-based detection (scanner)
- LLM-powered deep analysis (analyzer)
- Intelligent validation (validator)
- Comprehensive reporting (reporter)
"""

import sys
import os
from pathlib import Path
import yaml
from openai import OpenAI
import logging
from datetime import datetime
import time
import json

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

# Setup logging
def setup_logging():
    """Configure logging for the test."""
    log_dir = Path(__file__).parent / "logs"
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"detector_test_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"Detector test logging initialized. Log file: {log_file}")
    return logger

logger = setup_logging()

# Load OpenAI API key for LLM-powered analysis
llm_config = Path(__file__).parent.parent.parent / "llm.yml"
if llm_config.exists():
    logger.info(f"Loading OpenAI configuration from: {llm_config}")
    with open(llm_config) as f:
        config = yaml.safe_load(f)
        api_key = config.get("openai-key", "")
        client = OpenAI(api_key=api_key)
    logger.info("OpenAI client initialized for LLM-powered analysis")
else:
    logger.warning(f"Configuration file not found: {llm_config}")
    client = None


def query_llm(prompt: str, model: str = "gpt-4") -> str:
    """Query OpenAI with a prompt."""
    if not client:
        return "LLM not available"
    
    logger.info(f"Querying LLM for intelligent analysis (model: {model})")
    start_time = time.time()
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=2000
        )
        
        elapsed = time.time() - start_time
        tokens = response.usage.total_tokens if hasattr(response, 'usage') else 0
        logger.info(f"LLM analysis successful - Time: {elapsed:.2f}s, Tokens: {tokens}")
        
        return response.choices[0].message.content
    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"LLM query failed after {elapsed:.2f}s: {str(e)}")
        return f"Error: {e}"


def print_section(title: str):
    """Print formatted section header."""
    print("\n" + "="*80)
    print(title)
    print("="*80 + "\n")


def scan_with_patterns(project_path: Path) -> dict:
    """Pattern-based vulnerability scanning."""
    logger.info(f"Starting pattern-based scan of: {project_path}")
    
    vulnerabilities = []
    patterns = {
        'sql_injection': r'.*\+.*["\'].*SELECT|INSERT|UPDATE|DELETE.*',
        'xss': r'response\.getWriter\(\)\.print|response\.getWriter\(\)\.write',
        'xxe': r'DocumentBuilderFactory\.newInstance\(\)',
        'path_traversal': r'new File\(.*\+.*\)',
        'weak_crypto': r'DES|MD5|SHA1(?![\d])',
        'hardcoded_creds': r'password\s*=\s*["\'][^"\']+["\']',
        'command_injection': r'Runtime\.getRuntime\(\)\.exec|ProcessBuilder',
        'deserialization': r'ObjectInputStream|readObject',
    }
    
    java_files = list(project_path.rglob("*.java"))
    logger.info(f"Found {len(java_files)} Java files to scan")
    
    for java_file in java_files:
        try:
            with open(java_file, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
                
                for vuln_type, pattern in patterns.items():
                    import re
                    for i, line in enumerate(lines, 1):
                        if re.search(pattern, line, re.IGNORECASE):
                            vulnerabilities.append({
                                'type': vuln_type,
                                'file': java_file.name,
                                'line': i,
                                'code': line.strip(),
                                'severity': 'high' if vuln_type in ['sql_injection', 'xxe', 'command_injection'] else 'medium'
                            })
            logger.debug(f"Scanned {java_file.name}")
        except Exception as e:
            logger.error(f"Error scanning {java_file}: {e}")
    
    logger.info(f"Pattern scan complete: {len(vulnerabilities)} potential vulnerabilities found")
    return {
        'success': True,
        'findings': vulnerabilities,
        'files_scanned': len(java_files)
    }


def analyze_with_llm(vulnerabilities: list) -> dict:
    """LLM-powered deep analysis of vulnerabilities."""
    logger.info(f"Starting LLM-powered analysis of {len(vulnerabilities)} findings")
    
    # Group by file for analysis
    by_file = {}
    for vuln in vulnerabilities:
        file = vuln['file']
        if file not in by_file:
            by_file[file] = []
        by_file[file].append(vuln)
    
    analyzed_findings = []
    
    for file, vulns in by_file.items():
        logger.info(f"Analyzing {len(vulns)} vulnerabilities in {file}")
        
        vuln_summary = "\n".join([
            f"- {v['type']} (line {v['line']}): {v['code'][:60]}..."
            for v in vulns[:5]  # Limit to first 5 per file
        ])
        
        prompt = f"""You are a security expert analyzing Java code vulnerabilities.

File: {file}

Detected vulnerabilities:
{vuln_summary}

For each vulnerability, provide:
1. **Severity**: Critical/High/Medium/Low with justification
2. **Exploitability**: How easy it is to exploit (1-5, 5=easiest)
3. **Impact**: What damage could be done
4. **Confidence**: How confident are you this is a real vulnerability (1-100%)
5. **Priority**: Should this be fixed immediately? (Yes/No)
6. **Quick Fix**: One-sentence fix recommendation

Be concise but specific. Format as JSON array."""

        analysis = query_llm(prompt)
        
        # Try to parse LLM response
        try:
            if "```json" in analysis:
                json_str = analysis.split("```json")[1].split("```")[0].strip()
                llm_analysis = json.loads(json_str)
            else:
                llm_analysis = [{"raw_analysis": analysis}]
        except:
            llm_analysis = [{"raw_analysis": analysis}]
        
        # Merge LLM analysis with original findings
        for i, vuln in enumerate(vulns):
            enhanced_vuln = vuln.copy()
            if i < len(llm_analysis):
                enhanced_vuln['llm_analysis'] = llm_analysis[i]
            analyzed_findings.append(enhanced_vuln)
    
    logger.info(f"LLM analysis complete: {len(analyzed_findings)} findings enhanced")
    return {
        'success': True,
        'findings': analyzed_findings
    }


def validate_findings(findings: list) -> dict:
    """Validate findings to reduce false positives."""
    logger.info(f"Validating {len(findings)} findings")
    
    validated = []
    false_positives = []
    
    for finding in findings:
        # Simple validation logic - in production, this would be more sophisticated
        is_valid = True
        
        # Check if it's in test files (likely false positive)
        if 'test' in finding['file'].lower():
            is_valid = False
            false_positives.append(finding)
        else:
            validated.append(finding)
    
    logger.info(f"Validation complete: {len(validated)} confirmed, {len(false_positives)} false positives")
    return {
        'success': True,
        'validated_findings': validated,
        'false_positives': false_positives
    }


def generate_report(findings: list) -> dict:
    """Generate comprehensive security report with LLM."""
    logger.info(f"Generating comprehensive report for {len(findings)} findings")
    
    # Summarize findings
    by_severity = {}
    by_type = {}
    
    for f in findings:
        severity = f.get('severity', 'unknown')
        vuln_type = f.get('type', 'unknown')
        
        by_severity[severity] = by_severity.get(severity, 0) + 1
        by_type[vuln_type] = by_type.get(vuln_type, 0) + 1
    
    # Use LLM to generate executive summary
    summary_prompt = f"""You are a security consultant writing an executive summary.

Analysis Results:
- Total vulnerabilities found: {len(findings)}
- By severity: {json.dumps(by_severity, indent=2)}
- By type: {json.dumps(by_type, indent=2)}

Write a concise executive summary (3-4 paragraphs) covering:
1. Overall security posture
2. Most critical issues
3. Recommended priority actions
4. Estimated remediation effort

Be business-focused and actionable."""

    executive_summary = query_llm(summary_prompt)
    
    # Generate detailed report
    report = f"""
# Java Security Analysis Report

**Generated**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Files Analyzed**: {len(set(f['file'] for f in findings))}
**Total Findings**: {len(findings)}

## Executive Summary

{executive_summary}

## Statistics

### By Severity
{json.dumps(by_severity, indent=2)}

### By Vulnerability Type
{json.dumps(by_type, indent=2)}

## Detailed Findings

"""
    
    for i, finding in enumerate(findings[:10], 1):  # First 10 for brevity
        report += f"""
### Finding {i}: {finding['type'].replace('_', ' ').title()}

- **File**: {finding['file']}
- **Line**: {finding['line']}
- **Severity**: {finding['severity']}
- **Code**: `{finding['code']}`
"""
        if 'llm_analysis' in finding:
            report += f"\n**Analysis**: {json.dumps(finding['llm_analysis'], indent=2)}\n"
        report += "\n---\n"
    
    logger.info("Report generation complete")
    return {
        'success': True,
        'report': report,
        'summary': executive_summary,
        'statistics': {
            'by_severity': by_severity,
            'by_type': by_type
        }
    }


def main():
    """Run complete intelligent detection test."""
    logger.info("="*80)
    logger.info("Starting JavaSecurityDetector Intelligent Test")
    logger.info("="*80)
    
    print("="*80)
    print("üîí JavaSecurityDetector - Intelligent Analysis Test")
    print("="*80)
    print("\nThis test demonstrates the complete detection pipeline:")
    print("  1. Pattern-based scanning")
    print("  2. LLM-powered deep analysis")
    print("  3. Intelligent validation")
    print("  4. Comprehensive reporting")
    print()
    
    start_time = time.time()
    
    # Locate test project
    project_path = Path(__file__).parent / "test_project" / "src" / "main" / "java" / "com" / "example" / "vulnerable"
    
    if not project_path.exists():
        logger.error(f"Test project not found: {project_path}")
        print(f"‚ùå Error: Test project not found at {project_path}")
        return
    
    logger.info(f"Target project: {project_path}")
    print(f"üìÅ Target: {project_path.name}/")
    print()
    
    # Phase 1: Scan
    print_section("Phase 1: Pattern-Based Vulnerability Scanning")
    logger.info("Starting Phase 1: Scanning")
    
    scan_result = scan_with_patterns(project_path)
    if not scan_result['success']:
        logger.error("Scan phase failed")
        print("‚ùå Scan failed")
        return
    
    findings = scan_result['findings']
    print(f"‚úÖ Scan complete:")
    print(f"   - Files scanned: {scan_result['files_scanned']}")
    print(f"   - Potential vulnerabilities: {len(findings)}")
    print(f"   - Types: {', '.join(set(f['type'] for f in findings))}")
    
    # Phase 2: Analyze with LLM
    if client and findings:
        print_section("Phase 2: LLM-Powered Deep Analysis")
        logger.info("Starting Phase 2: LLM Analysis")
        
        analysis_result = analyze_with_llm(findings)
        if analysis_result['success']:
            findings = analysis_result['findings']
            print(f"‚úÖ Deep analysis complete:")
            print(f"   - Findings enhanced with LLM intelligence")
            print(f"   - Added severity, exploitability, and impact assessments")
    
    # Phase 3: Validate
    print_section("Phase 3: Intelligent Validation")
    logger.info("Starting Phase 3: Validation")
    
    validation_result = validate_findings(findings)
    validated_findings = validation_result['validated_findings']
    false_positives = validation_result['false_positives']
    
    print(f"‚úÖ Validation complete:")
    print(f"   - Confirmed vulnerabilities: {len(validated_findings)}")
    print(f"   - False positives removed: {len(false_positives)}")
    
    # Phase 4: Report
    print_section("Phase 4: Comprehensive Report Generation")
    logger.info("Starting Phase 4: Reporting")
    
    report_result = generate_report(validated_findings)
    
    print("‚úÖ Report generated")
    print(f"\n{report_result['summary']}\n")
    
    # Save report
    output_file = Path(__file__).parent / "DETECTOR_ANALYSIS_REPORT.md"
    logger.info(f"Saving report to: {output_file}")
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_result['report'])
        logger.info("Report saved successfully")
        print(f"üíæ Full report saved to: {output_file.name}")
    except Exception as e:
        logger.error(f"Failed to save report: {e}")
        print(f"‚ö†Ô∏è  Error saving report: {e}")
    
    elapsed = time.time() - start_time
    logger.info(f"Analysis complete in {elapsed:.2f} seconds")
    
    # Final summary
    print_section("‚úÖ Analysis Complete")
    print(f"""
Summary:
  üìÅ Files Analyzed: {scan_result['files_scanned']}
  üîç Vulnerabilities Found: {len(findings)}
  ‚úì Validated Findings: {len(validated_findings)}
  ‚è±Ô∏è  Total Time: {elapsed:.2f} seconds

Intelligence Features:
  ‚úÖ Pattern-based detection (fast baseline)
  ‚úÖ LLM-powered deep analysis (context-aware)
  ‚úÖ Intelligent validation (false positive reduction)
  ‚úÖ Executive-level reporting (business-focused)

This demonstrates the complete JavaSecurityDetector workflow with
both speed (pattern matching) and intelligence (LLM analysis)!
""")
    
    logger.info("Test completed successfully")
    print(f"\nüìù Detailed logs: logs/detector_test_*.log")


if __name__ == "__main__":
    logger.info("Application started")
    try:
        main()
        logger.info("Application finished successfully")
    except KeyboardInterrupt:
        logger.warning("Test interrupted by user")
        print("\n\n‚ö†Ô∏è  Test interrupted")
    except Exception as e:
        logger.error(f"Test error: {str(e)}", exc_info=True)
        print(f"\n\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
